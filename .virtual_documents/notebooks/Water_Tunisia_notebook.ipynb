import pandas as pd
import functions as func
import numpy as np
import yaml
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import GradientBoostingRegressor
from IPython.display import display
from scipy import stats
import plotly.express as px



import os
print(os.getcwd()) 


from functions import clean_data, feature_engineering, split_data, train_model, evaluate_model, plot_predictions





# Paths to your two data files
tunis_dams_rain_data_path = '../data/raw/tunis_dams_rain_mehdi.csv'
weather_data_path = '../data/raw/weather.csv'

def load_data(file_path):
    """Loads CSV file into a pandas DataFrame."""
    return pd.read_csv(file_path)
tunis_dams_rain_data = load_data(tunis_dams_rain_data_path)
weather_data = load_data(weather_data_path)
# Display a preview of each dataset
display(tunis_dams_rain_data.head())
display(weather_data.head())



# Display dataset previews
def preview_data(df, name):
    """Displays the first 5 rows of a DataFrame."""
    print(f"Preview of {name}:")
    display(df.head())

preview_data(tunis_dams_rain_data, 'tunis_dams_rain_data')
preview_data(weather_data, 'weather_data')



# Check for missing values
def check_missing_values(df, name):
    print(f"Checking for missing values in {name}:")
    print(df.isnull().sum())

check_missing_values(tunis_dams_rain_data, 'tunis_dams_rain_data')
check_missing_values(weather_data, 'weather_data')



# Step 2: Check for missing values
print("Checking for missing values in tunis_dams_rain_data:")
print(tunis_dams_rain_data.isnull().sum())

print("\nChecking for missing values in weather_data:")
print(weather_data.isnull().sum())



# Remove duplicates
def remove_duplicates(df, name):
    cleaned_df = df.drop_duplicates()
    print(f"{name} after removing duplicates:")
    display(cleaned_df.head())
    return cleaned_df

tunis_dams_rain_data_cleaned = remove_duplicates(tunis_dams_rain_data, 'tunis_dams_rain_data')
weather_data_cleaned = remove_duplicates(weather_data, 'weather_data')


# Check data types
def check_data_types(df, name):
    """Prints the data types of each column in a DataFrame."""
    print(f"Data types in {name}:")
    print(df.dtypes)

check_data_types(tunis_dams_rain_data_cleaned, 'tunis_dams_rain_data')
check_data_types(weather_data_cleaned, 'weather_data')



display(tunis_dams_rain_data_cleaned.head())



# Feature Engineering
def add_date_features(df, name):
    """Adds year, month, and day features from a 'date' column if present."""
    if 'date' in df.columns:
        df['Year'] = pd.DatetimeIndex(df['date']).year
        df['Month'] = pd.DatetimeIndex(df['date']).month
        df['Day'] = pd.DatetimeIndex(df['date']).day
        print(f"Added date features to {name}.")
    return df

def add_rainfall_feature(df):
    """Adds a cumulative rainfall feature if 'rainfall' column exists."""
    if 'rainfall' in df.columns:
        df['Cumulative_Rainfall'] = df['rainfall'].cumsum()
        print("Added 'Cumulative_Rainfall' feature.")
    return df

def add_temperature_change_feature(df):
    """Adds a temperature change feature if 'temperature' column exists."""
    if 'temperature' in df.columns:
        df['Temperature_Change'] = df['temperature'].diff()
        print("Added 'Temperature_Change' feature.")
    return df



# Display the updated datasets to verify the new features
print("Updated tunis_dams_rain_data with new features:")
display(tunis_dams_rain_data_cleaned.head())

print("\nUpdated weather_data with new features:")
display(weather_data_cleaned.head())





# Fill all NaN values with 0 in both datasets
tunis_dams_rain_data_cleaned = tunis_dams_rain_data_cleaned.fillna(0)
weather_data_cleaned = weather_data_cleaned.fillna(0)

# Display the datasets after handling NaN values
print("tunis_dams_rain_data after handling NaN values (filled with 0):")
display(tunis_dams_rain_data_cleaned.head())

print("\nweather_data after handling NaN values (filled with 0):")
display(weather_data_cleaned.head())


#Filling all NaN values with 0: This code replaces all missing (NaN) values in both DataFrames (tunis_dams_rain_data_cleaned and weather_data_cleaned) with 0.
#This ensures that no missing data remains, which can be useful for models that don't handle missing values well.
#It prevents the distortion of any future calculations or analyses due to missing data.



# Step: Join the two datasets by the common attribute 'date'

# Merge the two datasets on the 'date' column
merged_data = pd.merge(tunis_dams_rain_data_cleaned, weather_data_cleaned, on='date', how='inner')

# Display the merged dataset
print("Merged dataset based on 'date':")
display(merged_data.head())



# Load the cleaned weather and rain datasets
weather_data = pd.read_csv('../data/clean/weather_cleaned.csv')
rain_data = pd.read_csv('../data/clean/tunis_dams_rain_cleaned.csv')

# Check the column names to ensure the date columns are correctly named
print("Weather Data Columns:", weather_data.columns)
print("Rain Data Columns:", rain_data.columns)

weather_data['date'] = pd.to_datetime(weather_data['date'])  # If the column is named differently, adjust here
rain_data['date'] = pd.to_datetime(rain_data['date'])        # Same for the rain dataset

# Display the first few rows to verify the data
print(weather_data.head())
print(rain_data.head())

# Now we should be able to merge the datasets based on the date column
merged_data = pd.merge(rain_data, weather_data, on='date', how='left')

# Show the first few rows of the merged dataset
print(merged_data.head())



# Rename the merged dataframe to df_main
df_main = merged_data

# Create a new column for the average water level across all dams
df_main['average_water_level'] = df_main[['MELLEGUE', 'BEN METIR', 'KASSEB', 'BARBARA']].mean(axis=1)

# Display the updated dataset with the new column
display(df_main.head())



















#Bivariate Analysis: We are visualizing the relationship between the average water level and the average temperature using a scatterplot.
#Correlation: We calculate the correlation coefficient between these two variables to check for a linear relationbship.


import matplotlib.pyplot as plt
import seaborn as sns

# Assigning the merged dataset to df_main (if merged_data is already created)
df_main = merged_data  # Make sure merged_data exists and has the correct columns

# Bivariate Analysis: Scatterplot between average water level and average temperature (tavg)
plt.figure(figsize=(10, 6))
sns.scatterplot(x='MELLEGUE', y='tavg', data=df_main)  # Adjust column names if necessary
plt.title('Scatterplot of Average Water Level vs. Average Temperature')
plt.xlabel('Average Water Level (mm)')
plt.ylabel('Average Temperature (°C)')
plt.show()

# Calculate correlation between average water level and temperature
correlation = df_main['MELLEGUE'].corr(df_main['tavg'])  # Adjust column names if necessary
print(f'Correlation between average water level and temperature: {correlation}')






# Load the merged and cleaned dataset
df_main = pd.read_csv('/Users/mehdisahraoui/Desktop/IronHackordner/Iron_Hack_Final_Project/Iron_Hack_Water_in_Tunisia/data/clean/main.csv')

# Ensure 'average_water_level' exists
if 'average_water_level' not in df_main.columns:
    df_main['average_water_level'] = df_main[['MELLEGUE', 'BEN METIR', 'KASSEB', 'BARBARA']].mean(axis=1)

# Step 3: Multivariate Analysis

# Beispiel 1: Pairplot zur Visualisierung von Beziehungen zwischen mehreren Variablen
plt.figure(figsize=(12, 10))
sns.pairplot(df_main[['average_water_level', 'tavg', 'wspd', 'pres']]) # 'Month']], hue='Month')
plt.suptitle('Multivariate Analysis: Pairplot of Average Water Level, Temperature, Wind Speed, and Pressure', y=1.02)
plt.show()

# Beispiel 2: Heatmap der Korrelationsmatrix
plt.figure(figsize=(10, 8))
correlation_matrix = df_main[['average_water_level', 'tavg', 'wspd', 'pres']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()



# Convert the date column to datetime format if it's not already
df_main['date'] = pd.to_datetime(df_main['date'])

# Create a new column that categorizes the data into the first six months or the last six months
df_main['half_of_year'] = df_main['date'].dt.month.apply(lambda x: 'First Half' if x <= 6 else 'Second Half')

# Verify the new column
df_main[['date', 'half_of_year']].head()



# Select a subset of columns for the correlation matrix (focusing on relevant features)
columns_of_interest = ['MELLEGUE', 'KASSEB', 'SIDI SALEM', 'average_water_level', 'tavg', 'tmin', 'tmax', 'prcp', 'wspd']

# Calculate the correlation matrix for the selected columns
subset_corr_matrix = df_main[columns_of_interest].corr()

# Plot the heatmap with a larger figure size and adjusted font size for better readability
plt.figure(figsize=(10, 8))
sns.heatmap(subset_corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".2f", annot_kws={"size": 10})
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(fontsize=10)
plt.title('Heatmap of Correlation for Selected Variables', fontsize=14)
plt.tight_layout()  # To ensure everything fits
plt.show()



# Select only numeric columns for the correlation analysis
numeric_df = df_main.select_dtypes(include=[np.number])

# Calculate the correlation matrix for numeric columns
correlation_matrix = numeric_df.corr()

# Define the column representing water levels (e.g., 'MELLEGUE')
water_level_column = 'MELLEGUE'

# Get the correlation with the water level column
corr_with_water_level = correlation_matrix[water_level_column].sort_values(ascending=False)

# Display the correlation values
print(f"Correlation with '{water_level_column}':")
print(corr_with_water_level)



# Check for missing values in the columns
df_main[['snow', 'wpgt', 'tsun']].isna().sum()

# Check for the data types of these columns
print(df_main[['snow', 'wpgt', 'tsun']].dtypes)

# Check if these columns have only a single unique value
print(df_main[['snow', 'wpgt', 'tsun']].nunique())



df_main = df_main.drop(columns=['snow', 'wpgt', 'tsun'])






#Select a subset of columns for correlation analysis
columns_of_interest = ['MELLEGUE', 'SIDI SALEM', 'tavg', 'tmin', 'tmax', 'prcp', 'wspd']
subset_corr_matrix = df_main[columns_of_interest].corr()

# Plot the heatmap with fewer variables
plt.figure(figsize=(12, 8))
sns.heatmap(subset_corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".2f")
plt.title('Heatmap of Correlation for Selected Variables')
plt.show()






# Check column names to verify the correct water level column
print(df_main.columns)

# Select features (X) and target variable (y)
X = df_main[['tavg', 'tmin', 'tmax', 'prcp', 'wspd']]  # Example features
y = df_main['SILIANA']  # Using 'SILIANA' as the target variable due to the 0.697587 correlation

# Split data into training and test sets (80% training, 20% testing)
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)




# Define the folder where the cleaned data should be stored
clean_data_path = '../data/clean'

# Create the directory if it does not exist
if not os.path.exists(clean_data_path):
    os.makedirs(clean_data_path)

# Define the path to save the cleaned data
merged_data_path = os.path.join(clean_data_path, 'main.csv')

# Check if the file already exists
if not os.path.exists(merged_data_path):
    # Save the cleaned data
    merged_data.to_csv(merged_data_path, index=False)
    print(f"Cleaned and merged data has been saved to: {merged_data_path}")
else:
    print(f"The file '{merged_data_path}' already exists. Saving will be skipped.")



#Handling NaN values: We filled NaNs with 0 to make the analysis consistent.
#Removing duplicates: We removed duplicate entries to avoid redundancy.
#Correct data types: Ensuring correct data types is crucial for accurate calculations, especially with date data.


#naming the new Data frame and saving it 

# Create the clean data directory if it doesn't exist
if not os.path.exists(clean_data_path):
    os.makedirs(clean_data_path)

# Define the path to save the main data
main_data_path = os.path.join(clean_data_path, 'main.csv')

# Rename the merged dataframe to df_main
df_main = merged_data

# Ensure df_main exists
if 'df_main' in globals():
    # Save the df_main DataFrame to the clean folder
    df_main.to_csv(main_data_path, index=False)
    print(f"Main data has been saved to: {main_data_path}")
else:
    print("Error: 'df_main' is not defined. Make sure the DataFrame was successfully created.")






# Create a new column for the average water level across all dams
df_main['average_water_level'] = df_main[['MELLEGUE', 'BEN METIR', 'KASSEB', 'BARBARA']].mean(axis=1)

# Univariate Analysis for the average water level
plt.figure(figsize=(10, 6))
sns.histplot(df_main['average_water_level'], kde=True, bins=30)
plt.title('Distribution of Average Water Levels across All Dams')
plt.xlabel('Water Level (mm)')
plt.ylabel('Frequency')
plt.show()



# Load your cleaned dataset
df = pd.read_csv('../../data/clean/cleaned_dataset_with_half_of_year.csv')

# Univariate Analysis: Histograms for continuous variables
plt.figure(figsize=(10, 6))
df[['tavg', 'tmin', 'tmax', 'prcp', 'wspd']].hist(bins=30, figsize=(15, 10))
plt.suptitle("Univariate Analysis - Distribution of Continuous Variables", fontsize=16)
plt.show()

# Boxplots for detecting outliers
plt.figure(figsize=(10, 6))
sns.boxplot(data=df[['tavg', 'tmin', 'tmax', 'prcp', 'wspd']])
plt.title("Univariate Analysis - Boxplot of Continuous Variables", fontsize=16)
plt.show()





# Bivariate Analysis: Scatter plots to check relationships between continuous variables and target
plt.figure(figsize=(15, 8))

# Temperature vs. Water Level
plt.subplot(2, 2, 1)
sns.scatterplot(x='tavg', y='average_water_level', data=df)
plt.title("Temperature vs Water Level")

# Rainfall vs. Water Level
plt.subplot(2, 2, 2)
sns.scatterplot(x='prcp', y='average_water_level', data=df)
plt.title("Rainfall vs Water Level")

# Wind Speed vs. Water Level
plt.subplot(2, 2, 3)
sns.scatterplot(x='wspd', y='average_water_level', data=df)
plt.title("Wind Speed vs Water Level")

plt.tight_layout()
plt.show()

# Correlation Heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df[['tavg', 'tmin', 'tmax', 'prcp', 'wspd', 'average_water_level']].corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap", fontsize=16)
plt.show()






sns.pairplot(df[['tavg', 'tmin', 'tmax', 'prcp', 'wspd', 'average_water_level']], diag_kind='kde')
plt.suptitle("Multivariate Analysis - Pairplot of Continuous Variables", fontsize=16)
plt.show()



# Create a time-based index if not already done
df['date'] = pd.to_datetime(df['date'])

# Seasonal Analysis: Boxplot to show seasonal differences
plt.figure(figsize=(10, 6))
sns.boxplot(x='half_of_year', y='average_water_level', data=df)
plt.title("Seasonal Analysis - Water Levels by Season", fontsize=16)
plt.show()

# Temporal Analysis: Time series plot of water levels over time
plt.figure(figsize=(10, 6))
df.set_index('date')['average_water_level'].plot()
plt.title("Temporal Analysis - Water Levels Over Time", fontsize=16)
plt.xlabel("Date")
plt.ylabel("Water Levels")
plt.show()






# Hypothesis testing: Does rainfall significantly affect water levels?
# Null hypothesis: Rainfall has no effect on water levels.
# Alternative hypothesis: Rainfall has a significant effect on water levels.

# Pearson correlation test
corr, p_value = stats.pearsonr(df['prcp'], df['average_water_level'])
print(f"Pearson correlation between rainfall and water levels: {corr}")
print(f"P-value: {p_value}")

# If p_value < 0.05, we reject the null hypothesis
if p_value < 0.05:
    print("We reject the null hypothesis. Rainfall has a significant effect on water levels.")
else:
    print("We fail to reject the null hypothesis. Rainfall does not have a significant effect on water levels.")



print(tunis_dams_rain_data_cleaned.columns)






import pandas as pd
import os

# Use relative paths (relative to the location of this script)
weather_cleaned_path = '../data/clean/weather_cleaned.csv'
tunis_dams_rain_cleaned_path = '../data/clean/tunis_dams_rain_cleaned.csv'

# Load the datasets using pandas
weather_cleaned = pd.read_csv(weather_cleaned_path)
tunis_dams_rain_data_cleaned = pd.read_csv(tunis_dams_rain_cleaned_path)





# Set 'date' as the index for both DataFrames if not already set
if 'date' in tunis_dams_rain_data_cleaned.columns:
    tunis_dams_rain_data_cleaned.set_index('date', inplace=True)

if 'date' in weather_cleaned.columns:
    weather_cleaned.set_index('date', inplace=True)

# Check if 'date' index is in datetime format, and convert if necessary
if not pd.api.types.is_datetime64_any_dtype(tunis_dams_rain_data_cleaned.index):
    tunis_dams_rain_data_cleaned.index = pd.to_datetime(tunis_dams_rain_data_cleaned.index)

if not pd.api.types.is_datetime64_any_dtype(weather_cleaned.index):
    weather_cleaned.index = pd.to_datetime(weather_cleaned.index)

# Display the first few rows of the index to confirm conversion
print("\nFirst few 'date' values in tunis_dams_rain_data_cleaned index:")
print(tunis_dams_rain_data_cleaned.index[:5])

print("\nFirst few 'date' values in weather_cleaned index:")
print(weather_cleaned.index[:5])

# Proceed with further operations...
# Example: Creating new features
tunis_dams_rain_data_cleaned['month'] = tunis_dams_rain_data_cleaned.index.month
tunis_dams_rain_data_cleaned['season'] = tunis_dams_rain_data_cleaned['month'].apply(lambda x: 'First Half' if x <= 6 else 'Second Half')

# Creating lag features for water levels (example: MELLEGUE)
tunis_dams_rain_data_cleaned['lag_1'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(1)  # Lag by 1 day
tunis_dams_rain_data_cleaned['lag_7'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(7)  # Lag by 7 days (weekly effect)
tunis_dams_rain_data_cleaned['lag_30'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(30)  # Lag by 30 days (monthly effect)

# Drop any NaNs created by lagging
tunis_dams_rain_data_cleaned.dropna(inplace=True)

# Display the dataset to verify
print(tunis_dams_rain_data_cleaned.head())


# Check for column names in both DataFrames
print(tunis_dams_rain_data_cleaned.columns)
print(weather_cleaned.columns)

# If 'date' exists, convert it to datetime
if 'date' in tunis_dams_rain_data_cleaned.columns:
    tunis_dams_rain_data_cleaned['date'] = pd.to_datetime(tunis_dams_rain_data_cleaned['date'])
else:
    print("The 'date' column is missing in tunis_dams_rain_data_cleaned")

if 'date' in weather_cleaned.columns:
    weather_cleaned['date'] = pd.to_datetime(weather_cleaned['date'])
else:
    print("The 'date' column is missing in weather_cleaned")

# Check the first few rows of 'date' if conversion was successful
if 'date' in tunis_dams_rain_data_cleaned.columns:
    print(tunis_dams_rain_data_cleaned['date'].head())

if 'date' in weather_cleaned.columns:
    print(weather_cleaned['date'].head())
    



from statsmodels.tsa.seasonal import seasonal_decompose

# Assuming 'MELLEGUE' is the correct water level column
result = seasonal_decompose(tunis_dams_rain_data_cleaned['MELLEGUE'], model='additive', period=12)

# Plot the decomposition
result.plot()
plt.show()



import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets (using relative paths as discussed earlier)
weather_cleaned_path = '../data/clean/weather_cleaned.csv'
tunis_dams_rain_cleaned_path = '../data/clean/tunis_dams_rain_cleaned.csv'

weather_cleaned = pd.read_csv(weather_cleaned_path)
tunis_dams_rain_data_cleaned = pd.read_csv(tunis_dams_rain_cleaned_path)

# Set 'date' as index
weather_cleaned.set_index('date', inplace=True)
tunis_dams_rain_data_cleaned.set_index('date', inplace=True)

# Ensure that 'date' is in datetime format
weather_cleaned.index = pd.to_datetime(weather_cleaned.index)
tunis_dams_rain_data_cleaned.index = pd.to_datetime(tunis_dams_rain_data_cleaned.index)

# Merge the datasets on the 'date' index to create df_main
df_main = weather_cleaned.merge(tunis_dams_rain_data_cleaned, left_index=True, right_index=True)

# Ensure that columns 'tavg' and 'average_water_level' exist in the merged DataFrame (df_main)
df_main['average_water_level'] = df_main[['MELLEGUE', 'SILIANA']].mean(axis=1)  # Example calculation
df_main['tavg'] = weather_cleaned['tavg']  # Ensure this column exists in the weather_cleaned data

# Bivariate Analysis: Scatterplot between average water level and average temperature (tavg)
plt.figure(figsize=(10, 6))
sns.scatterplot(x='tavg', y='average_water_level', data=df_main)
plt.title('Scatterplot of Average Water Level vs. Average Temperature')
plt.xlabel('Average Temperature (°C)')
plt.ylabel('Average Water Level (mm)')
plt.show()

# Calculate correlation between average water level and temperature
correlation = df_main['average_water_level'].corr(df_main['tavg'])
print(f'Correlation between average water level and temperature: {correlation}')



# Extract the trend, seasonal, and residual components
trend = result.trend
seasonal = result.seasonal
residual = result.resid

# Visualize each component individually for better understanding
plt.figure(figsize=(10, 6))

plt.subplot(3, 1, 1)
plt.plot(trend, label='Trend')
plt.title('Trend Component')
plt.legend(loc='upper left')

plt.subplot(3, 1, 2)
plt.plot(seasonal, label='Seasonal', color='orange')
plt.title('Seasonal Component')
plt.legend(loc='upper left')

plt.subplot(3, 1, 3)
plt.plot(residual, label='Residual', color='green')
plt.title('Residual Component')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()



print(tunis_dams_rain_data_cleaned.columns)
















# No need to convert 'date' to datetime again since it's already the index.
# We can convert the index directly if necessary.
tunis_dams_rain_data_cleaned.index = pd.to_datetime(tunis_dams_rain_data_cleaned.index)

# Now create month and seasonal features from the datetime index
tunis_dams_rain_data_cleaned['month'] = tunis_dams_rain_data_cleaned.index.month
tunis_dams_rain_data_cleaned['season'] = tunis_dams_rain_data_cleaned['month'].apply(lambda x: 'First Half' if x <= 6 else 'Second Half')

# Creating lag features for water levels (example: MELLEGUE)
tunis_dams_rain_data_cleaned['lag_1'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(1)  # Lag by 1 day
tunis_dams_rain_data_cleaned['lag_7'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(7)  # Lag by 7 days (weekly effect)
tunis_dams_rain_data_cleaned['lag_30'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(30)  # Lag by 30 days (monthly effect)

# Drop any NaNs created by lagging
tunis_dams_rain_data_cleaned.dropna(inplace=True)

# Display the dataset to verify
display(tunis_dams_rain_data_cleaned.head())



print("Available columns:", tunis_dams_rain_data_cleaned.columns)



# Check the available columns to ensure 'date' exists
print("Available columns:", tunis_dams_rain_data_cleaned.columns)

# If 'date' column exists, proceed with the transformation
if 'date' in tunis_dams_rain_data_cleaned.columns:
    # Make a copy of the dataset for manipulation
    cleaned_dataset_with_half_of_year = tunis_dams_rain_data_cleaned.copy()

    # Convert the 'date' column to datetime format
    cleaned_dataset_with_half_of_year['date'] = pd.to_datetime(cleaned_dataset_with_half_of_year['date'])

    # Set 'date' as the index
    cleaned_dataset_with_half_of_year.set_index('date', inplace=True)

    # Create year, month, day features from the 'date' column
    cleaned_dataset_with_half_of_year['Year'] = cleaned_dataset_with_half_of_year.index.year
    cleaned_dataset_with_half_of_year['Month'] = cleaned_dataset_with_half_of_year.index.month
    cleaned_dataset_with_half_of_year['Day'] = cleaned_dataset_with_half_of_year.index.day

    # Display the updated dataset
    print(cleaned_dataset_with_half_of_year.head())

else:
    print("The dataset does not contain a 'date' column.")



weather_cleaned_path = '../data/clean/weather_cleaned.csv'
tunis_dams_rain_cleaned_path = '../data/clean/tunis_dams_rain_cleaned.csv'

# Load datasets
weather_cleaned = pd.read_csv(weather_cleaned_path)
tunis_dams_rain_data_cleaned = pd.read_csv(tunis_dams_rain_cleaned_path)

# Ensure date is the index
weather_cleaned.set_index('date', inplace=True)
tunis_dams_rain_data_cleaned.set_index('date', inplace=True)

# Merge datasets on date (as an example, adjust this part based on your actual workflow)
cleaned_dataset_with_half_of_year = weather_cleaned.merge(tunis_dams_rain_data_cleaned, left_index=True, right_index=True)

# Now proceed with your code
# Reset the index to make 'date' a regular column again
cleaned_dataset_with_half_of_year.reset_index(inplace=True)

# Move 'date' column to the first position (left side)
cols = cleaned_dataset_with_half_of_year.columns.tolist()
cols.insert(0, cols.pop(cols.index('date')))
cleaned_dataset_with_half_of_year = cleaned_dataset_with_half_of_year[cols]

# Display the dataset with 'date' at the left side
print(cleaned_dataset_with_half_of_year.head())


# Verify if 'date' exists and convert it to datetime
if 'date' in cleaned_dataset_with_half_of_year.columns:
    cleaned_dataset_with_half_of_year['date'] = pd.to_datetime(cleaned_dataset_with_half_of_year['date'])
    print("Date column converted successfully.")
else:
    print("Error: 'date' column not found.")

# Ensure that the 'X' DataFrame is properly defined
# Assuming 'X' is derived from the cleaned_dataset_with_half_of_year DataFrame
X = cleaned_dataset_with_half_of_year.drop(columns=['MELLEGUE'])  # Drop target column from features

# Create year, month, and day features from the 'date' column
X['Year'] = cleaned_dataset_with_half_of_year['date'].dt.year
X['Month'] = cleaned_dataset_with_half_of_year['date'].dt.month
X['Day'] = cleaned_dataset_with_half_of_year['date'].dt.day

# Display the first few rows to verify
print(X.head())



# One-hot encoding for categorical columns
X = pd.get_dummies(X, drop_first=True)

# Check the resulting features after encoding
print(X.head())



# Assuming tunis_dams_rain_data_cleaned already exists and has the 'MELLEGUE' column

# Creating lag features for water levels
tunis_dams_rain_data_cleaned['lag_1'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(1)  # Lag by 1 day
tunis_dams_rain_data_cleaned['lag_7'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(7)  # Lag by 7 days
tunis_dams_rain_data_cleaned['lag_30'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(30)  # Lag by 30 days

# Drop NaN rows created by lagging
tunis_dams_rain_data_cleaned.dropna(inplace=True)

# Define X (features) and y (target)
X = tunis_dams_rain_data_cleaned[['lag_1', 'lag_7', 'lag_30', 'SILIANA', 'RMIL', 'ABID', 'CHIBA']]  # Example features
y = tunis_dams_rain_data_cleaned['MELLEGUE']  # Target variable

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Model Evaluation
print("Random Forest R²:", r2_score(y_test, y_pred_rf))
print("Random Forest RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))


import matplotlib.pyplot as plt
import seaborn as sns

# Plot predicted vs actual
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred_rf)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # 45-degree line
plt.xlabel('Actual Water Levels')
plt.ylabel('Predicted Water Levels')
plt.title('Actual vs Predicted Water Levels - Random Forest')
plt.show()



import matplotlib.pyplot as plt
import seaborn as sns

# Calculate residuals
residuals = y_test - y_pred_rf

# Create a scatter plot of residuals
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=residuals)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residual Plot: Actual Water Levels vs Residuals')
plt.xlabel('Actual Water Levels')
plt.ylabel('Residuals')
plt.show()



# Defining X (features) and y (target)
# Including the relevant features, such as lag_1, lag_7, and others
X = tunis_dams_rain_data_cleaned[['lag_1', 'lag_7', 'lag_30', 'SILIANA', 'RMIL', 'ABID', 'CHIBA']]  # Add more relevant features here
y = tunis_dams_rain_data_cleaned['MELLEGUE']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Model Evaluation
print("Random Forest R²:", r2_score(y_test, y_pred_rf))
print("Random Forest RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))

# Extract feature importances
importances = rf_model.feature_importances_

# Create a DataFrame to hold the feature names and their corresponding importances
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': importances
})

# Sort the DataFrame by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance in Random Forest Model')
plt.show()



from sklearn.metrics import mean_absolute_error

# Defining X (features) and y (target)
# Including the relevant features, such as lag_1, lag_7, and others
X = tunis_dams_rain_data_cleaned[['lag_1', 'lag_7', 'lag_30', 'SILIANA', 'RMIL', 'ABID', 'CHIBA']]  # Add more relevant features here
y = tunis_dams_rain_data_cleaned['MELLEGUE']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Model Evaluation
print("Random Forest R²:", r2_score(y_test, y_pred_rf))
print("Random Forest RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_rf)
print(f"Random Forest MAE: {mae}")

# Extract feature importances
importances = rf_model.feature_importances_

# Create a DataFrame to hold the feature names and their corresponding importances
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': importances
})

# Sort the DataFrame by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)



import shap
shap.initjs()

# Initialize and train the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Initialize SHAP explainer for Random Forest
explainer = shap.TreeExplainer(rf_model)

# Calculate SHAP values for the test data
shap_values = explainer.shap_values(X_test)

# Plot the SHAP summary plot
shap.summary_plot(shap_values, X_test)

# If you want a more detailed view, you can plot individual feature impacts for a single prediction
# Example: SHAP force plot for a specific prediction
shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])












from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Select features (X) and target variable (y)
X = df_main[['tavg', 'tmin', 'tmax', 'prcp', 'wspd']]  # Example features
y = df_main['SILIANA']  # Using 'SILIANA' as the target variable

# Split data into training and test sets (80% training, 20% testing)
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Linear Regression model
model = LinearRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Display evaluation metrics
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R²): {r2}")



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import os

# Relative path to the main.csv file (adjust this based on where your script is located)
main_csv_path = '../data/clean/main.csv'

# Load the dataset using pandas
df_main = pd.read_csv(main_csv_path)

# Convert the 'date' column to datetime format
df_main['date'] = pd.to_datetime(df_main['date'])

# Function to map months to seasons
def get_season(month):
    if month in [12, 1, 2]:
        return 'Winter'
    elif month in [3, 4, 5]:
        return 'Spring'
    elif month in [6, 7, 8]:
        return 'Summer'
    else:
        return 'Autumn'

# Create a new 'season' column
df_main['season'] = df_main['date'].dt.month.apply(get_season)

# One-hot encode the 'season' column
df_main = pd.get_dummies(df_main, columns=['season'], drop_first=True)

# Check the data
print(df_main.head())

# Redefine X (features) to include the new season features
X = df_main[['tavg', 'tmin', 'tmax', 'prcp', 'wspd', 'season_Spring', 'season_Summer', 'season_Winter']]

# Redefine y (target)
y = df_main['SILIANA']

# Train-test split and model training as before
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Linear Regression model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Display evaluation metrics
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R²): {r2}")



#Handle Missing Values


# Ensure the 'date' column is in datetime format if it's not already
df_main['date'] = pd.to_datetime(df_main['date'])

# Extract the month from the 'date' column
df_main['month'] = df_main['date'].dt.month

# Reset index to avoid conflict with 'month' as both index and column
df_main = df_main.reset_index(drop=True)

# Fill NaN values with the mean value of the corresponding month
# Exclude the 'month' column from the filling operation
df_main.update(df_main.groupby('month').transform(lambda x: x.fillna(x.mean())))

# Check for columns that still have NaN values
cols_with_nan = df_main.columns[df_main.isnull().any()].tolist()

# Drop columns that still contain NaN values
df_main.drop(columns=cols_with_nan, inplace=True)

# Drop the 'month' column if it's no longer needed
df_main.drop(columns=['month'], inplace=True)

# Display the updated dataframe
print(df_main.head())



# Step 1: Define target (y) and features (X)
X = df_main[['tavg', 'tmin', 'tmax', 'prcp', 'wspd']]  # Example features
y = df_main['MELLEGUE']  # Target variable: MELLEGUE water level

# Step 2: Split the data into training and test sets (80% training, 20% testing)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Step 3: Initialize the model
model = LinearRegression()

# Step 4: Train the model on the training data
model.fit(X_train, y_train)

# Step 5: Make predictions on the test data
y_pred = model.predict(X_test)

# Step 6: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Step 7: Display evaluation metrics
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R²): {r2}")



# Create a new column to categorize into the first half and second half of the year
df_main['half_of_year'] = df_main['date'].dt.month.apply(lambda x: 'First Half' if x <= 6 else 'Second Half')

# Check the first few rows to verify the 'half_of_year' feature
df_main[['date', 'half_of_year']].head()



import os

# Define the relative path to the 'clean' directory and the file path
clean_dir = '../../data/clean'
file_path = f'{clean_dir}/cleaned_dataset_with_half_of_year.csv'

# Step 1: Check if the directory exists, if not, create it
if not os.path.exists(clean_dir):
    os.makedirs(clean_dir)
    print(f"Directory '{clean_dir}' created.")

# Step 2: Check if the file already exists, if not, create the new dataset with 'half_of_year' and save it
if not os.path.isfile(file_path):
    # Assuming df_main exists and contains your main data
    # Add the 'half_of_year' feature based on the 'date' column
    df_main['half_of_year'] = df_main['date'].apply(lambda x: 'First Half' if x.month <= 6 else 'Second Half')

    # Save the dataset with the 'half_of_year' feature to the clean directory
    df_main.to_csv(file_path, index=False)
    print(f"Dataset saved with 'half_of_year' feature at '{file_path}'.")
else:
    print(f"File '{file_path}' already exists, not overwriting.")




from sklearn.preprocessing import PolynomialFeatures

# Select the numerical features that could benefit from interaction and polynomial terms
selected_features = df_main[['tavg', 'wspd', 'pres']]

# Initialize PolynomialFeatures (degree=2 creates square and interaction terms)
poly = PolynomialFeatures(degree=2, include_bias=False)

# Fit and transform the selected features
poly_features = poly.fit_transform(selected_features)

# Automatically get feature names after fitting
poly_feature_names = poly.get_feature_names_out()

# Create a new DataFrame for the polynomial features
poly_df = pd.DataFrame(poly_features, columns=poly_feature_names)

# Concatenate the polynomial features with the original dataframe
df_main = pd.concat([df_main, poly_df], axis=1)

# Display the new columns to ensure that the polynomial features are added
print(df_main.columns)



# Display the first few rows with the new polynomial features
df_main[poly_feature_names].head()



# Drop duplicate columns
df_main = df_main.loc[:, ~df_main.columns.duplicated()]

# Display the cleaned dataframe
df_main.head()



# Drop duplicate columns
df_main = df_main.loc[:, ~df_main.columns.duplicated()]

# Display the cleaned dataframe
df_main.head()



from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Select relevant features and target for the model
X = df_main[['tavg', 'tmin', 'tmax', 'prcp', 'wspd', 'season_Spring', 'season_Summer', 'season_Winter', 'wspd pres', 'pres^2']]
y = df_main['SILIANA']  # Target variable

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize Ridge Regression model
ridge_model = Ridge(alpha=1.0)  # You can adjust the 'alpha' value to control regularization strength

# Train the model on the training data
ridge_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = ridge_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Display evaluation metrics
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R²): {r2}")



df_main['tavg_prcp_interaction'] = df_main['tavg'] * df_main['prcp']
df_main['tavg_wspd_interaction'] = df_main['tavg'] * df_main['wspd']



from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)
X_poly = poly.fit_transform(X)



rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)
feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)
print(feature_importances)



from sklearn.model_selection import GridSearchCV

param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0, 1000.0]}
grid_search = GridSearchCV(Ridge(), param_grid, scoring='r2', cv=5)
grid_search.fit(X_train, y_train)

best_alpha = grid_search.best_params_['alpha']
print("Best alpha for Ridge Regression:", best_alpha)





# Initialize the Ridge Regression model with the best alpha value
ridge_model = Ridge(alpha=100.0)

# Train the model on the training data
ridge_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred_ridge = ridge_model.predict(X_test)

# Evaluate the model
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

# Display evaluation metrics
print(f"Mean Squared Error (MSE): {mse_ridge}")
print(f"R-squared (R²): {r2_ridge}")




# Create interaction features
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X_train)

# Generate new feature names
poly_feature_names = poly.get_feature_names_out(X.columns)

# Convert the new features to a DataFrame
X_train_poly = pd.DataFrame(X_poly, columns=poly_feature_names)

# Do the same for the test set
X_test_poly = poly.transform(X_test)
X_test_poly = pd.DataFrame(X_test_poly, columns=poly_feature_names)



from scipy import stats

# Select only the numeric columns
numeric_cols = df_main.select_dtypes(include=[np.number])

# Calculate Z-scores for numeric columns to identify outliers (Threshold = 3)
z_scores = stats.zscore(numeric_cols)

# Find rows where any column's Z-score is greater than 3 (or less than -3) to detect outliers
outliers = (abs(z_scores) > 3).any(axis=1)

# Remove the outliers from the data
df_cleaned = df_main[~outliers]

# Display the cleaned dataset
print(df_cleaned.head())





# Ensure 'date' is in the right format if not already
df_main['date'] = pd.to_datetime(df_main['date'])

# Drop any non-numeric columns or categorical columns
df_main = df_main.select_dtypes(include=[np.number])

# Fill NaN values with forward fill or backward fill
df_main = df_main.ffill().bfill()

# Define relevant features for polynomial expansion
relevant_features = ['tavg', 'wspd', 'pres']  # example

# Generate polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(df_main[relevant_features])

# Get the polynomial feature names
poly_feature_names = poly.get_feature_names_out(relevant_features)

# Convert to DataFrame
X_poly_df = pd.DataFrame(X_poly, columns=poly_feature_names)

# Select the target variable (ensure it's numeric)
y = df_main['MELLEGUE']  # or any other target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_poly_df, y, test_size=0.3, random_state=42)

# Initialize and train Ridge Regression model with the best alpha
ridge_model = Ridge(alpha=100.0)
ridge_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_ridge = ridge_model.predict(X_test)

# Evaluate the Ridge Regression model
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

print(f"Mean Squared Error (MSE) with Ridge Regression: {mse_ridge}")
print(f"R-squared (R²) with Ridge Regression: {r2_ridge}")

# Plot the distributions of relevant features
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
sns.histplot(df_main['tavg'], kde=True)
plt.title('Distribution of tavg')

plt.subplot(1, 3, 2)
sns.histplot(df_main['wspd'], kde=True)
plt.title('Distribution of wspd')

plt.subplot(1, 3, 3)
sns.histplot(df_main['pres'], kde=True)
plt.title('Distribution of pres')

plt.tight_layout()
plt.show()



# Exclude 'MELLEGUE' and 'date' columns
X = tunis_dams_rain_data.drop(columns=['MELLEGUE', 'date'])

# Use 'MELLEGUE' as the target variable
y = tunis_dams_rain_data['MELLEGUE']

# Ensure only numeric columns in X
X = X.select_dtypes(include=[np.number])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Model Evaluation
print(f"Random Forest R²: {r2_score(y_test, y_pred_rf)}")

# Manually calculate RMSE
mse = mean_squared_error(y_test, y_pred_rf)
rmse = np.sqrt(mse)
print(f"Random Forest RMSE: {rmse}")

# Ensure correct sizes of y_test and y_pred_rf
print(f"y_test size: {len(y_test)}, y_pred_rf size: {len(y_pred_rf)}")

# Scatter plot of actual vs predicted values (with correctly sized arrays)
plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred_rf, alpha=0.5)
plt.title('Actual vs Predicted Water Levels')
plt.xlabel('Actual Water Levels')
plt.ylabel('Predicted Water Levels')
plt.show()



# Define the file path
file_path = '../../data/clean/cleaned_dataset_with_half_of_year.csv'

# Step 1: Load the dataset if it exists
if os.path.isfile(file_path):
    df_main = pd.read_csv(file_path)
    print(f"Dataset loaded from '{file_path}'.")
else:
    raise FileNotFoundError(f"File '{file_path}' does not exist. Please check the file path or generate the dataset first.")

# Step 2: One-hot encode the 'half_of_year' feature and include other features
X = pd.get_dummies(df_main[['tavg', 'tmin', 'tmax', 'prcp', 'wspd', 'half_of_year']], drop_first=True)
y = df_main['average_water_level']

# Step 3: Split data into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Initialize Ridge Regression model
ridge_model = Ridge(alpha=1.0)

# Step 5: Train the Ridge model
ridge_model.fit(X_train, y_train)

# Step 6: Predict on the test set
y_pred_ridge = ridge_model.predict(X_test)

# Step 7: Evaluate the model
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

# Step 8: Display the evaluation metrics
print(f"Ridge Regression with Seasonal Features - Mean Squared Error: {mse_ridge}")
print(f"Ridge Regression with Seasonal Features - R² Score: {r2_ridge}")

# Step 9: Plot Actual vs Predicted
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_ridge, alpha=0.5)
plt.xlabel('Actual Water Levels')
plt.ylabel('Predicted Water Levels')
plt.title('Actual vs Predicted Water Levels (Ridge Regression with Seasonal Features)')
plt.show()


cleaned_dataset_with_half_of_year = pd.DataFrame({
    'season': ['summer', 'winter', 'summer', 'winter', 'spring'],
    'MELLEGUE': [100, 200, 150, 175, 160],
    'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'],
    'feature1': [10.5, 20.5, 15.5, 18.5, 16.5],
    'feature2': [30.1, 25.3, 27.8, 22.6, 26.7]
})

# Assuming 'season' is the categorical column, use one-hot encoding
cleaned_dataset_with_half_of_year_encoded = pd.get_dummies(cleaned_dataset_with_half_of_year, columns=['season'])

# Drop non-numeric columns like 'date'
X = cleaned_dataset_with_half_of_year_encoded.drop(columns=['MELLEGUE', 'date'])  # Drop target column and 'date'
y = cleaned_dataset_with_half_of_year_encoded['MELLEGUE']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Calculate mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")


# Load the dataset
weather_data = pd.read_csv(weather_data_path)

# Check the available column names
print(weather_data.columns)












# Define the file path
file_path = '../../data/clean/cleaned_dataset_with_half_of_year.csv'

# Step 1: Load the dataset if it exists
if os.path.isfile(file_path):
    df_main = pd.read_csv(file_path)
    print(f"Dataset loaded from '{file_path}'.")
else:
    raise FileNotFoundError(f"File '{file_path}' does not exist. Please check the file path or generate the dataset first.")

# Step 2: One-hot encode the 'half_of_year' feature and include other features
X = pd.get_dummies(df_main[['tavg', 'tmin', 'tmax', 'prcp', 'wspd', 'half_of_year']], drop_first=True)
y = df_main['average_water_level']

# Step 3: Split data into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Initialize Ridge Regression model
ridge_model = Ridge(alpha=1.0)

# Step 5: Train the Ridge model
ridge_model.fit(X_train, y_train)

# Step 6: Predict on the test set
y_pred_ridge = ridge_model.predict(X_test)

# Step 7: Evaluate the model
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)

# Step 8: Display the evaluation metrics
print(f"Ridge Regression with Seasonal Features - Mean Squared Error: {mse_ridge}")
print(f"Ridge Regression with Seasonal Features - R² Score: {r2_ridge}")

# Step 9: Plot Actual vs Predicted
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_ridge, alpha=0.5)
plt.xlabel('Actual Water Levels')
plt.ylabel('Predicted Water Levels')
plt.title('Actual vs Predicted Water Levels (Ridge Regression with Seasonal Features)')
plt.show()



from sklearn.ensemble import GradientBoostingRegressor

# Define the file path
file_path = '../../data/clean/cleaned_dataset_with_half_of_year.csv'

# Step 1: Load the dataset if it exists
if os.path.isfile(file_path):
    df_main = pd.read_csv(file_path)
    print(f"Dataset loaded from '{file_path}'.")
else:
    raise FileNotFoundError(f"File '{file_path}' does not exist. Please check the file path or generate the dataset first.")

# Step 2: One-hot encode the 'half_of_year' feature and include other features
X = pd.get_dummies(df_main[['tavg', 'tmin', 'tmax', 'prcp', 'wspd', 'half_of_year']], drop_first=True)
y = df_main['average_water_level']

# Step 3: Split data into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Initialize Gradient Boosting Regressor
gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)

# Step 5: Train the model
gb_model.fit(X_train, y_train)

# Step 6: Predict on the test set
y_pred_gb = gb_model.predict(X_test)

# Step 7: Evaluate the model
mse_gb = mean_squared_error(y_test, y_pred_gb)
r2_gb = r2_score(y_test, y_pred_gb)

print(f"Gradient Boosting with Seasonal Features - Mean Squared Error: {mse_gb}")
print(f"Gradient Boosting with Seasonal Features - R² Score: {r2_gb}")

# Step 8: Plot Actual vs Predicted
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_gb, alpha=0.5)
plt.xlabel('Actual Water Levels')
plt.ylabel('Predicted Water Levels')
plt.title('Actual vs Predicted Water Levels (Gradient Boosting with Seasonal Features)')
plt.show()



# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10]
}

# Initialize the Gradient Boosting model
gboost = GradientBoostingRegressor(random_state=42)

# Setup Grid Search with 5-fold cross-validation
grid_search = GridSearchCV(estimator=gboost, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit the grid search to the training data
grid_search.fit(X_train, y_train)

# Get the best parameters and the best estimator
best_params = grid_search.best_params_
best_gboost = grid_search.best_estimator_

# Output the best hyperparameters found
print(f"Best hyperparameters found: {best_params}")



# Predict using the best Gradient Boosting model
y_pred_best_gboost = best_gboost.predict(X_test)

# Calculate the MSE and R^2 score
mse_best_gboost = mean_squared_error(y_test, y_pred_best_gboost)
r2_best_gboost = r2_score(y_test, y_pred_best_gboost)

# Print the results
print(f"Tuned Gradient Boosting – Mean Squared Error: {mse_best_gboost}")
print(f"Tuned Gradient Boosting – R² Score: {r2_best_gboost}")

# Plot Actual vs Predicted for the tuned model
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_best_gboost, alpha=0.6)
plt.title('Actual vs Predicted Water Levels (Tuned Gradient Boosting)')
plt.xlabel('Actual Water Levels')
plt.ylabel('Predicted Water Levels')
plt.show()




df_main['tavg'] = df_main['tavg'].fillna(df_main['tavg'].median())
df_main['tmin'] = df_main['tmin'].fillna(df_main['tmin'].median())
df_main['tmax'] = df_main['tmax'].fillna(df_main['tmax'].median())
df_main['prcp'] = df_main['prcp'].fillna(df_main['prcp'].median())
df_main['wspd'] = df_main['wspd'].fillna(df_main['wspd'].median())
df_main





# Define the parameter grid for Gradient Boosting
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5],
    'min_samples_split': [10, 20, 30]
}

# Initialize the Gradient Boosting Regressor
gbr = GradientBoostingRegressor(random_state=42)

# Set up GridSearchCV with 5-fold cross-validation
grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, 
                           scoring='neg_mean_squared_error', cv=5, n_jobs=-1)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print(f"Best hyperparameters found: {best_params}")



# Train the Gradient Boosting model with the best parameters
best_gbr_model = GradientBoostingRegressor(
    n_estimators=best_params['n_estimators'],
    learning_rate=best_params['learning_rate'],
    max_depth=best_params['max_depth'],
    min_samples_split=best_params['min_samples_split'],
    random_state=42
)

best_gbr_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_gbr = best_gbr_model.predict(X_test)

# Step 3: Evaluate the model
mse_gbr = mean_squared_error(y_test, y_pred_gbr)
r2_gbr = r2_score(y_test, y_pred_gbr)

print(f"Optimized Gradient Boosting - Mean Squared Error: {mse_gbr}")
print(f"Optimized Gradient Boosting - R² Score: {r2_gbr}")



from sklearn.preprocessing import StandardScaler

# Initialize the scaler
scaler = StandardScaler()

# Apply scaling to the features
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Use the scaled data in the Gradient Boosting Regressor
best_gbr_model.fit(X_train_scaled, y_train)
y_pred_scaled_gbr = best_gbr_model.predict(X_test_scaled)

# Evaluate the scaled model
mse_scaled_gbr = mean_squared_error(y_test, y_pred_scaled_gbr)
r2_scaled_gbr = r2_score(y_test, y_pred_scaled_gbr)

print(f"Scaled Gradient Boosting - Mean Squared Error: {mse_scaled_gbr}")
print(f"Scaled Gradient Boosting - R² Score: {r2_scaled_gbr}")



import matplotlib.pyplot as plt

# Plot Actual vs Predicted for Gradient Boosting
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_gbr, alpha=0.5, color='blue')
plt.xlabel('Actual Water Levels')
plt.ylabel('Predicted Water Levels')
plt.title('Actual vs Predicted Water Levels (Optimized Gradient Boosting)')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.show()



from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV

# Define the Gradient Boosting model
gbr = GradientBoostingRegressor(random_state=42)

# Define the parameter grid to search
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.05],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10]
}

# Set up the GridSearchCV to search for the best hyperparameters
grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')

# Train the model on the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print(f"Best hyperparameters found: {best_params}")

# Re-train the model with the best parameters
best_gbr_model = GradientBoostingRegressor(**best_params, random_state=42)
best_gbr_model.fit(X_train, y_train)

# Step 2: Predict on the test set and evaluate
y_pred_gbr = best_gbr_model.predict(X_test)

mse_gbr = mean_squared_error(y_test, y_pred_gbr)
r2_gbr = r2_score(y_test, y_pred_gbr)

print(f"Gradient Boosting - Mean Squared Error: {mse_gbr}")
print(f"Gradient Boosting - R² Score: {r2_gbr}")

# Plot Actual vs Predicted for Gradient Boosting
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_gbr, alpha=0.5)
plt.xlabel('Actual Water Levels')
plt.ylabel('Predicted Water Levels')
plt.title('Actual vs Predicted Water Levels (Gradient Boosting)')
plt.show()



# Check feature importance in Gradient Boosting
importances = best_gbr_model.feature_importances_
features = X_train.columns

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(features, importances)
plt.xlabel('Importance')
plt.ylabel('Features')
plt.title('Feature Importance in Gradient Boosting Model')
plt.show()









from sklearn.preprocessing import PolynomialFeatures

# Creating polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False) 
X_poly = poly.fit_transform(X)

print(f"Original features shape: {X.shape}")
print(f"Transformed features shape: {X_poly.shape}")



from xgboost import XGBRegressor

# Train XGBoost Regressor
xgb_model = XGBRegressor()
xgb_model.fit(X_train, y_train)

# Predictions and Evaluation
y_pred_xgb = xgb_model.predict(X_test)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost MSE: {mse_xgb}, R² Score: {r2_xgb}")



#Hyperparameter Tuning (Full Grid Search)
from sklearn.model_selection import GridSearchCV

param_grid_xgb = {
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 300, 500],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
}

grid_xgb = GridSearchCV(XGBRegressor(), param_grid_xgb, scoring='neg_mean_squared_error', cv=3, n_jobs=2)
grid_xgb.fit(X_train, y_train)

best_params_xgb = grid_xgb.best_params_
print(f"Best hyperparameters for XGBoost: {best_params_xgb}")




from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import cross_val_score

# Define the Gradient Boosting Regressor model
gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Train the model
gbr_model.fit(X_train, y_train)

# Perform cross-validation
cv_scores = cross_val_score(gbr_model, X, y, scoring='neg_mean_squared_error', cv=5)

# Print the mean and standard deviation of cross-validation scores
print(f"Mean CV MSE: {-cv_scores.mean()}, Std CV MSE: {cv_scores.std()}")



from sklearn.model_selection import cross_val_score

# Cross-validation with Gradient Boosting
cv_scores = cross_val_score(gbr_model, X, y, scoring='neg_mean_squared_error', cv=5)
print(f"Mean CV MSE: {-cv_scores.mean()}, Std CV MSE: {cv_scores.std()}")



print(f"y_test shape: {y_test.shape}")
print(f"y_pred shape: {y_pred.shape}")



train_columns = set(X_train.columns)
test_columns = set(X_test.columns)

# Find columns that are in one set but not the other
print("Columns in training set but not in test set:", train_columns - test_columns)
print("Columns in test set but not in training set:", test_columns - train_columns)



# Step 1: Sicherstellen, dass die Features korrekt transformiert werden
# Annahme: Du hast bereits 'half_of_year' in X_train und X_test kodiert (z.B. mit pd.get_dummies)
# Jetzt, überprüfe und angleiche die Spalten

# Missing columns in X_test
missing_columns = set(X_train.columns) - set(X_test.columns)
if missing_columns:
    print("Missing columns in X_test:", missing_columns)
    for col in missing_columns:
        X_test[col] = 0  # Standardwert für fehlende Spalten setzen

# Extra columns in X_test (Spalten, die in X_test vorhanden, aber nicht in X_train sind)
extra_columns = set(X_test.columns) - set(X_train.columns)
if extra_columns:
    print("Extra columns in X_test:", extra_columns)
    X_test.drop(columns=extra_columns, inplace=True)

# Step 2: Spaltenreihenfolge angleichen
X_test = X_test[X_train.columns]

# Step 3: Jetzt kannst du die Vorhersage mit dem Modell machen
try:
    y_pred = model.predict(X_test)
except Exception as e:
    print(f"Fehler bei der Vorhersage: {str(e)}")

# Step 4: Überprüfen, ob die Shapes übereinstimmen
if y_test.shape != y_pred.shape:
    print(f"Shape mismatch: y_test shape is {y_test.shape}, y_pred shape is {y_pred.shape}")
else:
    # Berechne Residuen und plotte sie, falls die Shapes übereinstimmen
    residuals = y_test - y_pred
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=residuals)
    plt.axhline(0, color='red', linestyle='--')
    plt.xlabel('Actual Water Levels')
    plt.ylabel('Residuals')
    plt.title('Residual Plot')
    plt.show()









import shap

# SHAP summary plot for Gradient Boosting model
explainer = shap.TreeExplainer(gbr_model)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test)



import os
import yaml

# Print the current working directory
print(f"Current working directory: {os.getcwd()}")

# Correct relative path from the notebooks folder to the root directory
config_path = os.path.join('..', 'config.yaml')

# Try loading the config.yaml file
try:
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)
        print("Config file loaded successfully.")
except FileNotFoundError:
    print(f"File {config_path} not found. Please check if the file exists in the correct directory.")

# Example of how to access values from the config.yaml
try:
    raw_data_path = config['data_paths']['raw_data']
    cleaned_data_path = config['data_paths']['cleaned_data']
    model_output_path = config['data_paths']['model_output']
    print(f"Raw data path: {raw_data_path}")
    print(f"Cleaned data path: {cleaned_data_path}")
    print(f"Model output path: {model_output_path}")
except KeyError as e:
    print(f"Missing key in config.yaml: {e}")









from sklearn.metrics import mean_absolute_error

# Defining X (features) and y (target)
# Including the relevant features, such as lag_1, lag_7, and others
X = tunis_dams_rain_data_cleaned[['lag_1', 'lag_7', 'lag_30', 'SILIANA', 'RMIL', 'ABID', 'CHIBA']]  # Add more relevant features here
y = tunis_dams_rain_data_cleaned['MELLEGUE']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Model Evaluation
print("Random Forest R²:", r2_score(y_test, y_pred_rf))
print("Random Forest RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_rf)
print(f"Random Forest MAE: {mae}")

# Extract feature importances
importances = rf_model.feature_importances_

# Create a DataFrame to hold the feature names and their corresponding importances
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': importances
})

# Sort the DataFrame by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)



rf_model = RandomForestRegressor(n_estimators=100, random_state=42)



from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='r2')
print(f"Cross-Validated R²: {cv_scores.mean()}")



# Check if training and test features match
print("Training Features:", X_train.columns)
print("Test Features:", X_test.columns)

# Ensure they are the same
assert list(X_train.columns) == list(X_test.columns), "Train and test feature names do not match!"












# Create lag features in the entire dataset
tunis_dams_rain_data_cleaned['lag_1'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(1)
tunis_dams_rain_data_cleaned['lag_7'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(7)
tunis_dams_rain_data_cleaned['lag_30'] = tunis_dams_rain_data_cleaned['MELLEGUE'].shift(30)

# Drop any rows with NaN values created by shifting
tunis_dams_rain_data_cleaned.dropna(inplace=True)

# Now define X and y after the feature engineering
X = tunis_dams_rain_data_cleaned[['lag_1', 'lag_7', 'lag_30', 'SILIANA', 'RMIL', 'ABID', 'CHIBA']]
y = tunis_dams_rain_data_cleaned['MELLEGUE']

# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)






# Train the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
print("Random Forest R²:", r2_score(y_test, y_pred_rf))
print("Random Forest RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))
print("Random Forest MAE:", mean_absolute_error(y_test, y_pred_rf))






















